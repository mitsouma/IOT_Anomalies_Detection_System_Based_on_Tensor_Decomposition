{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOJKCw6Bre0aNnO2znVvL/V",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsouma/IOT_Anomalies_Detection_System_Based_on_Tensor_Decomposition/blob/main/Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AS3YCKWRHUwK",
        "outputId": "59546e2d-5d8f-484f-c8fc-02297c8aaf62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorly in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorly) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tensorly) (1.15.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -U tensorly"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorly.decomposition import parafac\n",
        "import tensorly as tl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorly import unfold, fold\n",
        "from tensorly.tenalg import khatri_rao\n",
        "from numpy.linalg import lstsq\n",
        "from tensorly.cp_tensor import cp_to_tensor"
      ],
      "metadata": {
        "id": "evw0gpj7da6I"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AnomalyDetectionPipeline:\n",
        "    def __init__(self, config=None):\n",
        "        self.config = config or {\n",
        "            'tensor_rank': 10,        # Rang pour la décomposition CP\n",
        "            'window_size': 300,       # Fenêtre temporelle en secondes\n",
        "            'learning_rate': 0.001,\n",
        "            'batch_size': 32,\n",
        "            'epochs': 50\n",
        "        }\n",
        "        self.model = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def create_graph_tensor(self, df_normal, df_train, window_size=None):\n",
        "        \"\"\"\n",
        "        Crée deux tenseurs (normal et train) avec les mêmes dimensions\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df_normal : pd.DataFrame\n",
        "            DataFrame contenant les données normales\n",
        "        df_train : pd.DataFrame\n",
        "            DataFrame contenant les données d'entraînement\n",
        "        window_size : int, optional\n",
        "            Taille de la fenêtre temporelle en secondes\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        tensor_data : dict\n",
        "            Dictionnaire contenant les tenseurs et les mappings\n",
        "        \"\"\"\n",
        "        if window_size is None:\n",
        "            window_size = self.config['window_size']\n",
        "\n",
        "        # Combiner les dataframes pour extraire toutes les valeurs uniques\n",
        "        combined_df = pd.concat([df_normal, df_train], ignore_index=True)\n",
        "\n",
        "        # Extract unique source and destination IPs from both dataframes\n",
        "        src_ips = combined_df['Src IP'].unique()\n",
        "        dst_ips = combined_df['Dst IP'].unique()\n",
        "\n",
        "        # Create mappings for src_ip and dst_ip to indices\n",
        "        features = ['count', 'bytes', 'duration', 'packets']\n",
        "        src_ip_to_idx = {ip: idx for idx, ip in enumerate(src_ips)}\n",
        "        dst_ip_to_idx = {ip: idx for idx, ip in enumerate(dst_ips)}\n",
        "        idx_to_feature = {idx: feature for idx, feature in enumerate(features)}\n",
        "\n",
        "        # Create time windows for both dataframes\n",
        "        df_normal['time_window'] = df_normal['relative_time'].astype(np.int64) // (10**9 * window_size)\n",
        "        df_train['time_window'] = df_train['relative_time'].astype(np.int64) // (10**9 * window_size)\n",
        "\n",
        "        # Get all unique time windows from both dataframes\n",
        "        all_time_windows = sorted(pd.concat([\n",
        "            df_normal['time_window'], df_train['time_window']\n",
        "        ]).unique())\n",
        "\n",
        "        # Create adjacency tensors [time_windows, Src IP, Dst IP, features]\n",
        "        tensor_normal = np.zeros((len(all_time_windows), len(src_ips), len(dst_ips), 4), dtype=np.float32)\n",
        "        tensor_train = np.zeros((len(all_time_windows), len(src_ips), len(dst_ips), 4), dtype=np.float32)\n",
        "\n",
        "        # Define a function to fill a tensor\n",
        "        def fill_tensor(df, tensor):\n",
        "            time_window_to_idx = {tw: idx for idx, tw in enumerate(all_time_windows)}\n",
        "\n",
        "            # Group by time window, source and destination\n",
        "            for _, group in df.groupby(['time_window', 'Src IP', 'Dst IP']):\n",
        "                t_idx = time_window_to_idx[group['time_window'].iloc[0]]\n",
        "                src_idx = src_ip_to_idx[group['Src IP'].iloc[0]]\n",
        "                dst_idx = dst_ip_to_idx[group['Dst IP'].iloc[0]]\n",
        "\n",
        "                # Aggregated features\n",
        "                tensor[t_idx, src_idx, dst_idx, 0] = len(group)  # count\n",
        "                tensor[t_idx, src_idx, dst_idx, 1] = group['Flow Bytes/s'].sum()  # bytes\n",
        "                tensor[t_idx, src_idx, dst_idx, 2] = group['Flow Duration'].mean()  # duration\n",
        "                tensor[t_idx, src_idx, dst_idx, 3] = group['Flow Packets/s'].sum()  # packets\n",
        "\n",
        "        # Fill both tensors\n",
        "        fill_tensor(df_normal, tensor_normal)\n",
        "        fill_tensor(df_train, tensor_train)\n",
        "\n",
        "        return {\n",
        "            'normal': tensor_normal,\n",
        "            'train': tensor_train,\n",
        "            'src_ip_to_idx': src_ip_to_idx,\n",
        "            'dst_ip_to_idx': dst_ip_to_idx,\n",
        "            'time_windows': all_time_windows,\n",
        "            'mappings': {\n",
        "                'Src_IP': src_ip_to_idx,\n",
        "                'Dst_IP': dst_ip_to_idx,\n",
        "                'features_names':idx_to_feature\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def tensor_decomposition(self, tensor_normal):\n",
        "        \"\"\"\n",
        "        Étape 2: Décomposition du tenseur normal et extraction des composantes de faible rang\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        tensor_normal : np.ndarray\n",
        "            Tenseur normal à décomposer\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        decomposition_result : dict\n",
        "            Résultat de la décomposition (tenseur de faible rang et facteurs)\n",
        "        \"\"\"\n",
        "        # Utilise la décomposition CP pour obtenir un modèle de tenseur de faible rang\n",
        "        rank = self.config['tensor_rank']\n",
        "        factors = parafac(tensor_normal, rank=rank, init='random', tol=1e-6)\n",
        "\n",
        "        # Pour un tenseur 4D (Time × Src_IP × Dst_IP × Feature)\n",
        "        A, B, C, D = factors\n",
        "\n",
        "        # Reconstruire le tenseur avec les facteurs\n",
        "        low_rank_tensor = cp_to_tensor((None, [A, B, C, D]))\n",
        "\n",
        "        return {\n",
        "            'low_rank_tensor': low_rank_tensor,\n",
        "            'factors': factors,\n",
        "            'rank': rank\n",
        "        }\n",
        "\n",
        "    def normal_space_projection(self, tensor_train, decomposition_result):\n",
        "        \"\"\"\n",
        "        Étape 3: Projection du tenseur d'entraînement sur l'espace normal\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        tensor_train : np.ndarray\n",
        "            Tenseur d'entraînement à projeter\n",
        "        decomposition_result : dict\n",
        "            Résultat de la décomposition du tenseur normal\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        residuals : np.ndarray\n",
        "            Tenseur des résidus (anomalies potentielles)\n",
        "        \"\"\"\n",
        "        # Extraire les facteurs du tenseur normal\n",
        "        A, B, C, D = decomposition_result['factors']\n",
        "        rank = decomposition_result['rank']\n",
        "\n",
        "        # Nombre de tranches temporelles dans le tenseur d'entraînement\n",
        "        new_I = tensor_train.shape[0]\n",
        "\n",
        "        # Initialiser le nouveau facteur A pour le tenseur d'entraînement\n",
        "        new_A = np.zeros((new_I, rank))\n",
        "\n",
        "        # Pour chaque tranche temporelle du tenseur d'entraînement\n",
        "        for k in range(new_I):\n",
        "            # Déplier le tenseur dans son premier mode (temps)\n",
        "            # Reshape: 1 × (Src_IP × Dst_IP × Feature)\n",
        "            T_k_unfolded = tensor_train[k].reshape(1, -1)\n",
        "\n",
        "            # Calculer le produit de Khatri-Rao pour B, C, et D\n",
        "            # (D ⊙ C ⊙ B)\n",
        "            kr_product = khatri_rao([D, khatri_rao([C, B])])\n",
        "\n",
        "            # Résoudre pour trouver le nouveau vecteur a(k)\n",
        "            # a(k) = T_k_unfolded × ((D ⊙ C ⊙ B)ᵀ)†\n",
        "            new_A[k] = T_k_unfolded @ np.linalg.pinv(kr_product.T)\n",
        "\n",
        "        # Reconstruire le nouveau modèle avec le nouveau facteur A\n",
        "        V_hat = cp_to_tensor((None, [new_A, B, C, D]))\n",
        "\n",
        "        # Calculer les résidus pour le tenseur d'entraînement\n",
        "        residuals = tensor_train - V_hat\n",
        "\n",
        "        return residuals\n",
        "\n",
        "    def aggregation_scoring(self, residuals, mappings, time_windows): # Corrected indentation\n",
        "        \"\"\"\n",
        "        Step 4: Aggregate anomaly scores by fixing Dst IP, time window, and feature (but not Src IP)\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        residuals : np.ndarray\n",
        "            Residual tensor of shape (Time, Src IP, Dst IP, Feature)\n",
        "        mappings : dict\n",
        "            Mappings of dimension indices to real values (e.g., IP addresses)\n",
        "        time_windows : list\n",
        "            List of time windows corresponding to the time axis\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        scores_df : pd.DataFrame\n",
        "            DataFrame containing aggregated anomaly scores\n",
        "        \"\"\"\n",
        "\n",
        "        scores_data = []\n",
        "\n",
        "        # Reverse mappings\n",
        "        idx_to_dst_ip = {idx: ip for ip, idx in mappings['Dst_IP'].items()}\n",
        "        #Fix: Changed  idx_to_feature = {idx: feat for feat, idx in mappings['Feature'].items()} to  idx_to_feature = {idx: feat for feat, idx in mappings['features_names'].items()}\n",
        "        idx_to_feature = {idx: feat for feat, idx in mappings['features_names'].items()}\n",
        "\n",
        "        time_dim, src_dim, dst_dim, feat_dim = residuals.shape\n",
        "\n",
        "        # Loop over time, dst_ip, and feature — not src_ip\n",
        "        for t in range(time_dim):\n",
        "            time_window = time_windows[t]\n",
        "            for d in range(dst_dim):\n",
        "                dst_ip = idx_to_dst_ip[d]\n",
        "                for f in range(feat_dim):\n",
        "                    feature_name = idx_to_feature[f]\n",
        "\n",
        "                    # Slice over all Src IPs for this (time, dst, feature)\n",
        "                    values = residuals[t, :, d, f]  # shape: (Src IP,)\n",
        "\n",
        "                    # Compute aggregation metrics\n",
        "                    scores_data.append({\n",
        "                        'Time_window': time_window,\n",
        "                        'Dst_IP': dst_ip,\n",
        "                        'Feature': feature_name,\n",
        "                        'mean_abs_residual': np.mean(np.abs(values)),\n",
        "                        'max_abs_residual': np.max(np.abs(values)),\n",
        "                        'std_residual': np.std(values),\n",
        "                        'sum_squared_residual': np.sum(values ** 2)\n",
        "                    })\n",
        "\n",
        "        # Convert to DataFrame\n",
        "        scores_df = pd.DataFrame(scores_data)\n",
        "        return scores_df\n",
        "\n",
        "    def build_deep_learning_model(self, score_df):\n",
        "      \"\"\"\n",
        "      Step 5: Build a Deep Learning model using anomaly scores per Dst IP.\n",
        "\n",
        "      Parameters:\n",
        "      -----------\n",
        "      score_df : pd.DataFrame\n",
        "          DataFrame containing per-feature anomaly scores by Dst IP (and optionally labels).\n",
        "\n",
        "      Returns:\n",
        "      --------\n",
        "      dict\n",
        "          Dictionary with the trained model, input features, and learning mode.\n",
        "      \"\"\"\n",
        "      # Encode 'Dst IP'\n",
        "      score_df = pd.get_dummies(data=score_df,columns=['Dst IP'], dtype=int)\n",
        "      # Determine mode and prepare labels if needed\n",
        "      if 'label' in score_df.columns:\n",
        "          y = score_df['label'].values\n",
        "          mode = 'supervised'\n",
        "      else:\n",
        "          y = None\n",
        "          mode = 'unsupervised'\n",
        "\n",
        "      # Normalize input features\n",
        "      X_scaled = self.scaler.fit_transform(score_df.drop(columns=['label']))\n",
        "\n",
        "      # Build and train model\n",
        "      if mode == 'supervised':\n",
        "          model = tf.keras.Sequential([\n",
        "              tf.keras.layers.Dense(64, activation='relu', input_shape=(X_scaled.shape[1],)),\n",
        "              tf.keras.layers.Dropout(0.2),\n",
        "              tf.keras.layers.Dense(32, activation='relu'),\n",
        "              tf.keras.layers.Dropout(0.2),\n",
        "              tf.keras.layers.Dense(16, activation='relu'),\n",
        "              tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "          ])\n",
        "          model.compile(\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),\n",
        "              loss='binary_crossentropy',\n",
        "              metrics=['accuracy']\n",
        "          )\n",
        "          model.fit(\n",
        "              X_scaled, y,\n",
        "              batch_size=self.config['batch_size'],\n",
        "              epochs=self.config['epochs'],\n",
        "              validation_split=0.2,\n",
        "              verbose=1\n",
        "          )\n",
        "      else:\n",
        "          input_dim = X_scaled.shape[1]\n",
        "          encoding_dim = 8\n",
        "\n",
        "          input_layer = tf.keras.layers.Input(shape=(input_dim,))\n",
        "          encoded = tf.keras.layers.Dense(32, activation='relu')(input_layer)\n",
        "          encoded = tf.keras.layers.Dense(16, activation='relu')(encoded)\n",
        "          encoded = tf.keras.layers.Dense(encoding_dim, activation='relu')(encoded)\n",
        "\n",
        "          decoded = tf.keras.layers.Dense(16, activation='relu')(encoded)\n",
        "          decoded = tf.keras.layers.Dense(32, activation='relu')(decoded)\n",
        "          decoded = tf.keras.layers.Dense(input_dim, activation='sigmoid')(decoded)\n",
        "\n",
        "          model = tf.keras.Model(inputs=input_layer, outputs=decoded)\n",
        "          model.compile(\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=self.config['learning_rate']),\n",
        "              loss='mse'\n",
        "          )\n",
        "          model.fit(\n",
        "              X_scaled, X_scaled,\n",
        "              batch_size=self.config['batch_size'],\n",
        "              epochs=self.config['epochs'],\n",
        "              validation_split=0.2,\n",
        "              verbose=1\n",
        "          )\n",
        "\n",
        "          self.model = model\n",
        "          return {\n",
        "                'model': model,\n",
        "                'features': X_scaled,\n",
        "                'mode': mode\n",
        "            }\n",
        "\n",
        "\n",
        "\n",
        "    def run_pipeline(self, df_normal, df_train,rank, with_labels=False):\n",
        "        \"\"\"\n",
        "        Exécution complète du pipeline\n",
        "\n",
        "        Parameters:\n",
        "        -----------\n",
        "        df_normal : pd.DataFrame\n",
        "            DataFrame contenant les données normales\n",
        "        df_train : pd.DataFrame\n",
        "            DataFrame contenant les données d'entraînement\n",
        "        with_labels : bool, optional\n",
        "            Indique si les données d'entraînement contiennent des labels\n",
        "\n",
        "        Returns:\n",
        "        --------\n",
        "        results : dict\n",
        "            Résultats du pipeline\n",
        "        \"\"\"\n",
        "        # Étape 1: Construction des tenseurs\n",
        "        tensor_data = self.create_graph_tensor(df_normal, df_train)\n",
        "        tensor_normal = tensor_data['normal']\n",
        "        tensor_train = tensor_data['train']\n",
        "        mappings = tensor_data['mappings']\n",
        "        time_windows = tensor_data['time_windows']\n",
        "\n",
        "        # Étape 2: Décomposition du tenseur normal\n",
        "        rank=self.config['tensor_rank']\n",
        "        decomposition_result = self.tensor_decomposition(tensor_normal)\n",
        "\n",
        "        # Étape 3: Projection du tenseur d'entraînement sur l'espace normal\n",
        "        residuals = self.normal_space_projection(tensor_train, decomposition_result)\n",
        "\n",
        "        # Étape 4: Agrégation des scores\n",
        "        scores_df = self.aggregation_scoring(residuals, mappings, time_windows)\n",
        "\n",
        "        # Si les données d'entraînement contiennent des labels, les ajouter\n",
        "        if with_labels and 'Label' in df_train.columns:\n",
        "            # Créer un mapping pour les labels\n",
        "            label_map = {}\n",
        "            for _, row in df_train.iterrows():\n",
        "                time_window = row['relative_time'].astype(np.int64) // (10**9 * self.config['window_size'])\n",
        "                src_ip = row['Src IP']\n",
        "                dst_ip = row['Dst IP']\n",
        "                key = (time_window, src_ip, dst_ip)\n",
        "                # Supposons que 1 = attaque, 0 = normal\n",
        "                label_map[key] = 1 if row['Label'] == 'Attack' else 0\n",
        "\n",
        "            # Ajouter les labels au DataFrame des scores\n",
        "            def get_label(row):\n",
        "                key = (row['Time_window'], row['Src_IP'], row['Dst_IP'])\n",
        "                return label_map.get(key, 0)  # 0 pour normal par défaut\n",
        "\n",
        "            scores_df['label'] = scores_df.apply(get_label, axis=1)\n",
        "\n",
        "        # Étape 5: Construction et entraînement du modèle de Deep Learning\n",
        "        model_result = self.build_deep_learning_model(scores_df)\n",
        "\n",
        "        return {\n",
        "            'tensor_data': tensor_data,\n",
        "            'decomposition_result': decomposition_result,\n",
        "            'residuals': residuals,\n",
        "            'scores_df': scores_df,\n",
        "            'model_result': model_result\n",
        "        }\n",
        "\n",
        "# Exemple d'utilisation:\n",
        "# config = {\n",
        "#     'tensor_rank': 10,\n",
        "#     'window_size': 300,\n",
        "#     'learning_rate': 0.001,\n",
        "#     'batch_size': 32,\n",
        "#     'epochs': 50\n",
        "# }\n",
        "# pipeline = AnomalyDetectionPipeline(config)\n",
        "# results = pipeline.run_pipeline(df_normal, df_train, with_labels=True)"
      ],
      "metadata": {
        "id": "WDZayXrRdGzF"
      },
      "execution_count": 26,
      "outputs": []
    }
  ]
}