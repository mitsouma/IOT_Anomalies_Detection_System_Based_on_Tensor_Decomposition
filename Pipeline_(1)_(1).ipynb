{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mitsouma/IOT_Anomalies_Detection_System_Based_on_Tensor_Decomposition/blob/main/Pipeline_(1)_(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t8StKyQdSHn1"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorly.decomposition import parafac,non_negative_parafac\n",
        "import tensorly as tl\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "import tensorflow as tf\n",
        "from tensorly import unfold, fold\n",
        "from tensorly.tenalg import khatri_rao\n",
        "from numpy.linalg import lstsq\n",
        "from tensorly.cp_tensor import cp_to_tensor\n",
        "from sklearn.metrics import accuracy_score, log_loss\n",
        "import time\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
        "import torch\n",
        "from tensorflow.keras import layers, models, callbacks\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,classification_report\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NWBgfOkSFL1",
        "outputId": "7e304c30-6d2c-44bb-def0-300a8f93a61f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensorly\n",
            "  Downloading tensorly-0.9.0-py3-none-any.whl.metadata (8.6 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from tensorly) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from tensorly) (1.15.3)\n",
            "Downloading tensorly-0.9.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tensorly\n",
            "Successfully installed tensorly-0.9.0\n"
          ]
        }
      ],
      "source": [
        "pip install tensorly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qt-WNgMjimtR"
      },
      "source": [
        "\"\"\"\n",
        "This pipeline is designed to detect anomalies in network traffic using a tensor-based deep learning approach.\n",
        "\n",
        "Steps and Objectives:\n",
        "---------------------\n",
        "1. **Preprocessing & Tensor Construction**:\n",
        "   - Multivariate network traffic data is aggregated over time windows.\n",
        "   - A 4D tensor is built with dimensions: [Time_Window, Src IP, Dst IP,featues].\n",
        "\n",
        "2. **Tensor Decomposition (CP)**:\n",
        "   - The tensor is decomposed using CP (CANDECOMP/PARAFAC) to extract the underlying structure (normal behavior).\n",
        "   - This reveals low-rank patterns, which capture typical traffic patterns.\n",
        "\n",
        "3. **Reconstruction & Residual Calculation**:\n",
        "   - We take the training tenssor and we compute its CP decomposition\n",
        "   - Residuals are computed as the difference between the measure of the train tensor and the projection of this measure in the latent space.\n",
        "4. **Anomaly Scoring**:\n",
        "Since our goal aim to detect DDoS and DOS attack so:\n",
        "   - Scores are aggregated per destination IP (`Dst IP`) and feature.\n",
        "   - These scores reflect the degree of deviation from normal patterns.\n",
        "\n",
        "5. **Deep Learning Classification**:\n",
        "   - The aggregated anomaly scores are used as input to a neural network.\n",
        "   - If labeled data is available, the model is trained in a supervised way to classify normal vs anomalous flows.\n",
        "   - If labels are absent, an autoencoder is trained to learn normal behavior and detect deviations.\n",
        "\n",
        "6. **Prediction**:\n",
        "   - New traffic data is processed through the same pipeline.\n",
        "   - The trained model predicts anomaly scores or binary classifications.\n",
        "\n",
        "Main Goal:\n",
        "----------\n",
        "To detect abnormal network behavior in the IOT_data  by modeling traffic patterns as tensors, identifying deviations, and classifying them using deep learning models.\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "n1zTED3B1q4B"
      },
      "outputs": [],
      "source": [
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "A9_2WKh1tReb"
      },
      "outputs": [],
      "source": [
        "tl.set_backend('pytorch')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I21wW78hO0UF",
        "outputId": "f1404357-2c07-4eb2-a3f3-6e1c78bb136d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3fV3mESyd3P"
      },
      "source": [
        "**Define the dataset**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ep6pD1DF0PP9",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "df1=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Benign_flowbased/BenignTraffic.pcap_Flow.csv')\n",
        "df2=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Benign_flowbased/BenignTraffic1.pcap_Flow.csv')\n",
        "df3=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Benign_flowbased/BenignTraffic2.pcap_Flow.csv')\n",
        "df4=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Benign_flowbased/BenignTraffic3.pcap_Flow.csv')\n",
        "df5=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Anomalies_fbased/DDoS-HTTP_Flood-.pcap_Flow.csv')\n",
        "df6=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Anomalies_fbased/DoS-HTTP_Flood.pcap_Flow.csv')\n",
        "df7=pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Datasets_iot/Anomalies_fbased/DoS-HTTP_Flood1.pcap_Flow.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "dSa9PMXLReBV"
      },
      "outputs": [],
      "source": [
        "df1['label']=0\n",
        "df2['label']=0\n",
        "df3['label']=0\n",
        "df4['label']=0\n",
        "df5['label']=1\n",
        "df5['label']=1\n",
        "df6['label']=1\n",
        "df7['label']=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "WUdvqL8UVzeE"
      },
      "outputs": [],
      "source": [
        "df1.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df2.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df3.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df4.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df5.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df6.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df7.replace([np.inf, -np.inf], np.nan, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k59jI2602wF5",
        "outputId": "b3a22119-1244-404d-f39e-b76d47ff1892"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nan for :are : Flow Bytes/s      35\n",
            "Flow Packets/s    35\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      35\n",
            "Flow Packets/s    35\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      45\n",
            "Flow Packets/s    45\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      17\n",
            "Flow Packets/s    17\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      1123\n",
            "Flow Packets/s    1123\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      40786\n",
            "Flow Packets/s    40786\n",
            "dtype: int64\n",
            "nan for :are : Flow Bytes/s      38735\n",
            "Flow Packets/s    38735\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "#Drop na\n",
        "for df in [df1,df2,df3,df4,df5,df6,df7]:\n",
        "    r=df.isna().sum()\n",
        "    print(\"nan for :are :\",r[r>0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NP3XaH6Tzbmx",
        "outputId": "211d615f-2b89-477d-f78f-4736c10bf207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(183630, 85)\n",
            "(84526, 85)\n",
            "(91279, 85)\n",
            "(38895, 85)\n",
            "(505720, 85)\n",
            "(932513, 85)\n",
            "(710231, 85)\n"
          ]
        }
      ],
      "source": [
        "#Shape of each dataset\n",
        "print(df1.shape)\n",
        "print(df2.shape)\n",
        "print(df3.shape)\n",
        "print(df4.shape)\n",
        "print(df5.shape)\n",
        "print(df6.shape)\n",
        "print(df7.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "-q4JA-VT3B9U"
      },
      "outputs": [],
      "source": [
        "#Drop duplicated rows\n",
        "df1.drop_duplicates(inplace=True)\n",
        "df2.drop_duplicates(inplace=True)\n",
        "df3.drop_duplicates(inplace=True)\n",
        "df4.drop_duplicates(inplace=True)\n",
        "df5.drop_duplicates(inplace=True)\n",
        "df6.drop_duplicates(inplace=True)\n",
        "df7.drop_duplicates(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "flHDjg5aVRkh"
      },
      "outputs": [],
      "source": [
        "df1.dropna(inplace=True)\n",
        "df2.dropna(inplace=True)\n",
        "df3.dropna(inplace=True)\n",
        "df4.dropna(inplace=True)\n",
        "df5.dropna(inplace=True)\n",
        "df6.dropna(inplace=True)\n",
        "df7.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "7lsWA_FN1Xg1"
      },
      "outputs": [],
      "source": [
        "#Convert timestamp\n",
        "df1['Timestamp']=pd.to_datetime(df1['Timestamp'])\n",
        "df2['Timestamp']=pd.to_datetime(df2['Timestamp'])\n",
        "df3['Timestamp']=pd.to_datetime(df3['Timestamp'])\n",
        "df4['Timestamp']=pd.to_datetime(df4['Timestamp'])\n",
        "df5['Timestamp']=pd.to_datetime(df5['Timestamp'])\n",
        "df6['Timestamp']=pd.to_datetime(df6['Timestamp'])\n",
        "df7['Timestamp']=pd.to_datetime(df7['Timestamp'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "nb-pDgCL1xmh"
      },
      "outputs": [],
      "source": [
        "#Extract days\n",
        "df1['Jour']=df1['Timestamp'].dt.date\n",
        "df2['Jour']=df2['Timestamp'].dt.date\n",
        "df3['Jour']=df3['Timestamp'].dt.date\n",
        "df4['Jour']=df4['Timestamp'].dt.date\n",
        "df5['Jour']=df5['Timestamp'].dt.date\n",
        "df6['Jour']=df6['Timestamp'].dt.date\n",
        "df7['Jour']=df7['Timestamp'].dt.date"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "A__Mh_xC2WwI"
      },
      "outputs": [],
      "source": [
        "#Convert days\n",
        "df1['Jour']=pd.to_datetime(df1['Jour'])\n",
        "df2['Jour']=pd.to_datetime(df2['Jour'])\n",
        "df3['Jour']=pd.to_datetime(df3['Jour'])\n",
        "df4['Jour']=pd.to_datetime(df4['Jour'])\n",
        "df5['Jour']=pd.to_datetime(df5['Jour'])\n",
        "df6['Jour']=pd.to_datetime(df6['Jour'])\n",
        "df7['Jour']=pd.to_datetime(df7['Jour'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g1XhnESrdRTo",
        "outputId": "ec5f84c0-3102-44b6-cdb3-68787da38117"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<DatetimeArray>\n",
              "['2022-09-08 00:00:00']\n",
              "Length: 1, dtype: datetime64[ns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "df7['Jour'].unique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "8vof25FrdmyK"
      },
      "outputs": [],
      "source": [
        "df5_day_1=df5[df5['Jour']=='2022-09-14']\n",
        "df5_day_2=df5[df5['Jour']=='2022-11-07']\n",
        "df6_day_1=df6[df6['Jour']=='2022-08-08']\n",
        "df6_day_2=df6[df6['Jour']=='2022-09-08']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "21oWqcUF8QEi"
      },
      "outputs": [],
      "source": [
        "#Extract time\n",
        "df1['Heure']=df1['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df2['Heure']=df2['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df3['Heure']=df3['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df4['Heure']=df4['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df5_day_1['Heure']=df5_day_1['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df5_day_2['Heure']=df5_day_2['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df6_day_1['Heure']=df6_day_1['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df6_day_2['Heure']=df6_day_2['Timestamp'].dt.strftime('%H:%M:%S')\n",
        "df7['Heure']=df7['Timestamp'].dt.strftime('%H:%M:%S')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lQR3P8WG86m3"
      },
      "outputs": [],
      "source": [
        "#Create a relative time\n",
        "df1['relative_time']=df1['Timestamp']-df1['Timestamp'].min()\n",
        "df2['relative_time']=df2['Timestamp']-df2['Timestamp'].min()\n",
        "df3['relative_time']=df3['Timestamp']-df3['Timestamp'].min()\n",
        "df4['relative_time']=df4['Timestamp']-df4['Timestamp'].min()\n",
        "df5_day_1['relative_time']=df5_day_1['Timestamp']-df5_day_1['Timestamp'].min()\n",
        "df5_day_2['relative_time']=df5_day_2['Timestamp']-df5_day_2['Timestamp'].min()\n",
        "df6_day_1['relative_time']=df6_day_1['Timestamp']-df6_day_1['Timestamp'].min()\n",
        "df6_day_2['relative_time']=df6_day_2['Timestamp']-df6_day_2['Timestamp'].min()\n",
        "df7['relative_time']=df7['Timestamp']-df7['Timestamp'].min()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0ihKJeh-ReBZ"
      },
      "outputs": [],
      "source": [
        "#Normal filter\n",
        "hours = df1['relative_time'].dt.total_seconds() / 3600\n",
        "mask_3h = hours <= 1\n",
        "df_3h = df1[mask_3h]\n",
        "#Train filter\n",
        "hours_t = df4['relative_time'].dt.total_seconds() / 3600\n",
        "mask_3h_t = hours_t <= 1\n",
        "df_3h_t = df4[mask_3h_t]\n",
        "#Test filter\n",
        "hours_3h_test = df3['relative_time'].dt.total_seconds() / 3600\n",
        "mask_3h_test = hours_3h_test <= 1\n",
        "df_3h_test = df3[mask_3h_test]\n",
        "#Anomalous filter\n",
        "hours_3h_test_a = df5_day_2['relative_time'].dt.total_seconds() / 3600\n",
        "mask_3h_test_a = hours_3h_test_a <= 1\n",
        "df_3h_test_a = df5_day_2[mask_3h_test_a]\n",
        "df7_2h=df7[df7['relative_time']<pd.Timedelta(hours=1)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVbABgFxReBa"
      },
      "source": [
        "# DATA CONSTRUCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Cb6Fz4k55MeD"
      },
      "outputs": [],
      "source": [
        "#Consider for as normal_data \"Benign 1 \":\n",
        "df_normal=df_3h\n",
        "#Training data:\n",
        "df_train=pd.concat([df_3h_t,df5_day_1,df7_2h],axis=0)\n",
        "df_test=pd.concat([df_3h_test,df_3h_test_a,df6_day_2],axis=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qP9XfFhWReBb"
      },
      "source": [
        "# NORMALISE SOME COLUMNS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiwrxPo5ReBc"
      },
      "source": [
        "# TENSOR CONSTRUCTION"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "XsIamgsYReBc"
      },
      "outputs": [],
      "source": [
        "def build_global_ip_mappings(*dfs):\n",
        "    all_src_ips = set()\n",
        "    all_dst_ips = set()\n",
        "\n",
        "    for df in dfs:\n",
        "        all_src_ips.update(df['Src IP'].unique())\n",
        "        all_dst_ips.update(df['Dst IP'].unique())\n",
        "\n",
        "    src_ip_to_idx = {ip: idx for idx, ip in enumerate(sorted(all_src_ips))}\n",
        "    dst_ip_to_idx = {ip: idx for idx, ip in enumerate(sorted(all_dst_ips))}\n",
        "\n",
        "    return src_ip_to_idx, dst_ip_to_idx"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_ip_to_idx, dst_ip_to_idx=build_global_ip_mappings(df_normal,df_train,df_test)"
      ],
      "metadata": {
        "id": "TRCy6pTcKmQo"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_graph_tensor(df, src_ip_to_idx, dst_ip_to_idx, window_size=300):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "\n",
        "    df['time_window'] = df['relative_time'].astype(np.int64) // (10**9 * window_size)\n",
        "    time_windows = sorted(df['time_window'].unique())\n",
        "\n",
        "\n",
        "    df['count'] = df.groupby(['time_window', 'Src IP', 'Dst IP'])['Dst IP'].transform('count')\n",
        "\n",
        "    # StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "    features_to_normalize = ['count', 'Flow Bytes/s', 'Flow Duration', 'Flow Packets/s']\n",
        "    df[features_to_normalize] = scaler.fit_transform(df[features_to_normalize])\n",
        "\n",
        "\n",
        "    tensor_cpu = torch.zeros((len(time_windows), len(src_ip_to_idx), len(dst_ip_to_idx), 4), dtype=torch.float32)\n",
        "\n",
        "\n",
        "    for _, group in df.groupby(['time_window', 'Src IP', 'Dst IP']):\n",
        "        t_idx = np.where(np.array(time_windows) == group['time_window'].iloc[0])[0][0]\n",
        "        src_ip = group['Src IP'].iloc[0]\n",
        "        dst_ip = group['Dst IP'].iloc[0]\n",
        "\n",
        "        if src_ip in src_ip_to_idx and dst_ip in dst_ip_to_idx:\n",
        "            src_idx = src_ip_to_idx[src_ip]\n",
        "            dst_idx = dst_ip_to_idx[dst_ip]\n",
        "\n",
        "            tensor_cpu[t_idx, src_idx, dst_idx, 0] = group['count'].iloc[0]                     # count (déjà normalisé)\n",
        "            tensor_cpu[t_idx, src_idx, dst_idx, 1] = group['Flow Bytes/s'].sum()                # bytes/s (normalisé)\n",
        "            tensor_cpu[t_idx, src_idx, dst_idx, 2] = group['Flow Duration'].mean()              # duration (normalisé)\n",
        "            tensor_cpu[t_idx, src_idx, dst_idx, 3] = group['Flow Packets/s'].sum()              # packets/s (normalisé)\n",
        "\n",
        "    tensor = tensor_cpu.to(device)\n",
        "    print(\"✅ Tensor well created | shape:\", tensor.shape)\n",
        "    return tensor, time_windows\n"
      ],
      "metadata": {
        "id": "wbaB1KG5KsBW"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Supposons que tu as déjà chargé tes trois DataFrames\n",
        "# df_normal, df_train, df_test\n",
        "\n",
        "tensor_normal, time_normal = create_graph_tensor(df_normal, src_ip_to_idx, dst_ip_to_idx)\n",
        "tensor_train, time_train = create_graph_tensor(df_train, src_ip_to_idx, dst_ip_to_idx)\n",
        "tensor_test, time_test = create_graph_tensor(df_test, src_ip_to_idx, dst_ip_to_idx)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFhniTgEK4ch",
        "outputId": "d55e0a22-c029-4e58-a5a5-cf904602cef4"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "✅ Tensor well created | shape: torch.Size([13, 822, 1313, 4])\n",
            "Using device: cpu\n",
            "✅ Tensor well created | shape: torch.Size([12, 822, 1313, 4])\n",
            "Using device: cpu\n",
            "✅ Tensor well created | shape: torch.Size([13, 822, 1313, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_tensor_feature(tensor, feature_index):\n",
        "    # Extract the feature slice\n",
        "    feature_slice = tensor[:, :, :, feature_index]\n",
        "\n",
        "    # Calculate min and max\n",
        "    min_val = torch.min(feature_slice)\n",
        "    max_val = torch.max(feature_slice)\n",
        "\n",
        "    # Apply min-max normalization\n",
        "    if max_val - min_val > 0:\n",
        "        normalized_slice = (feature_slice - min_val) / (max_val - min_val)\n",
        "    else:\n",
        "        # All values are the same\n",
        "        normalized_slice = feature_slice - min_val  # results in all zeros\n",
        "\n",
        "    # Replace the original feature with the normalized one\n",
        "    tensor[:, :, :, feature_index] = normalized_slice\n",
        "\n",
        "    return tensor\n"
      ],
      "metadata": {
        "id": "E3dIXqlXyEbk"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Unpack the tuple into separate variables\n",
        "#tensor_normal, src_ip_to_idx, dst_ip_to_idx, time_windows = create_graph_tensor(df_normal, window_size=300)\n",
        "\n",
        "# Now normalize the tensor object\n",
        "tensor_normal = normalize_tensor_feature(tensor_normal, 0)\n",
        "#tensor_normal = normalize_tensor_feature(tensor_normal, 1)\n",
        "#tensor_normal = normalize_tensor_feature(tensor_normal, 2)\n",
        "#tensor_normal = normalize_tensor_feature(tensor_normal, 3)\n",
        "\n",
        "# If you need to update the original 'tensor_data' variable to contain the modified tensor\n",
        "# and the other elements, you can create a new tuple:\n",
        "\n",
        "# Repeat the unpacking and normalization for tensor_data_normal and tensor_data_test as well\n",
        "#tensor_train, src_ip_to_idx_train, dst_ip_to_idx_train, time_windows_train = create_graph_tensor(df_train, window_size=300)\n",
        "tensor_train = normalize_tensor_feature(tensor_train, 0)\n",
        "#tensor_train = normalize_tensor_feature(tensor_train, 1)\n",
        "#tensor_train = normalize_tensor_feature(tensor_train, 2)\n",
        "#tensor_train = normalize_tensor_feature(tensor_train, 3)\n",
        "\n",
        "#tensor_test, src_ip_to_idx_test, dst_ip_to_idx_test, time_windows_test = create_graph_tensor(df_test, window_size=300)\n",
        "tensor_test = normalize_tensor_feature(tensor_test, 0)\n",
        "#tensor_test = normalize_tensor_feature(tensor_test, 1)\n",
        "#tensor_test = normalize_tensor_feature(tensor_test, 2)\n",
        "#tensor_test = normalize_tensor_feature(tensor_test, 3)"
      ],
      "metadata": {
        "id": "ps5PBlZeyLoo"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVbn19AYReBc"
      },
      "source": [
        "# Tensor Normalisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H63ysbBA0Fdu"
      },
      "outputs": [],
      "source": [
        "factors_normal=parafac(tensor_normal, rank=1, init='random', n_iter_max=100)\n",
        "reconstruction= cp_to_tensor(factors_normal)\n",
        "# Calcul de l'erreur de reconstruction\n",
        "error = tl.norm(tensor_normal - reconstruction) / tl.norm(tensor_normal)\n",
        "print(f\"✅ Rank : {9}, Error : {error.item():.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK54spOBv81l"
      },
      "source": [
        "**Normal Projection**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def latent_space_projection(tensor_input, tensor_ref):\n",
        "\n",
        "    lambdas_ref, (A_ref, B_ref, C_ref, D_ref) = tensor_ref\n",
        "    rank = lambdas_ref.shape[0]\n",
        "\n",
        "    device = tensor_input.device\n",
        "    tensor_input = tensor_input.to(torch.float32).to(device)\n",
        "    B_ref = B_ref.to(torch.float32).to(device)\n",
        "    C_ref = C_ref.to(torch.float32).to(device)\n",
        "    D_ref = D_ref.to(torch.float32).to(device)\n",
        "    lambdas_ref = lambdas_ref.to(torch.float32).to(device)\n",
        "\n",
        "\n",
        "    kr_product = tl.tenalg.khatri_rao([D_ref, tl.tenalg.khatri_rao([C_ref, B_ref])])\n",
        "    kr_pinv = torch.linalg.pinv(kr_product.T)  # pseudo-inverse  Khatri-Rao\n",
        "\n",
        "    num_time_slices = tensor_input.shape[0]\n",
        "    A_proj = torch.zeros((num_time_slices, rank), dtype=torch.float32, device=device)\n",
        "\n",
        "    for t in range(num_time_slices):\n",
        "        slice_t =tensor_input[t].reshape(1, -1)\n",
        "        A_proj[t] = slice_t @ kr_pinv\n",
        "    low_rank_projected = cp_to_tensor((torch.ones(rank, device=device), [A_proj, B_ref, C_ref, D_ref]))\n",
        "    residual = tensor_input - low_rank_projected\n",
        "\n",
        "    print(\"✅ Décomposition, projection et résidu calculés.\")\n",
        "    print(\"Norme du résidu :\", tl.norm(residual).item())\n",
        "\n",
        "    return residual"
      ],
      "metadata": {
        "id": "5MF6PI3ursXi"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals_train = latent_space_projection(tensor_test, factors_normal)\n",
        "print(\"✅ Residual calculé.\")"
      ],
      "metadata": {
        "id": "50l6735ftOFg",
        "outputId": "428281e8-0df5-4151-ac42-7dfe3817e38a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Décomposition, projection et résidu calculés.\n",
            "Norme du résidu : 4119.30859375\n",
            "✅ Residual calculé.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nva5k2YIReBh"
      },
      "source": [
        "# AGGREGATION_SCORING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "RD68F03dReBi"
      },
      "outputs": [],
      "source": [
        "def aggregation_scoring_Source(residuals, mappings, df_with_labels, window_size=300):\n",
        "    print(\"⏳ Étape 4 : Aggrégation des scores avec labels...\")\n",
        "\n",
        "    # 1. Convertir relative_time -> time_window (entier)\n",
        "    df_with_labels = df_with_labels.copy()\n",
        "    df_with_labels['time_window'] = df_with_labels['relative_time'].astype(np.int64) // (10**9 * window_size)\n",
        "\n",
        "    # 2. Créer le mapping des labels (Dst IP, time_window) -> label\n",
        "    label_map = {}\n",
        "    for _, row in df_with_labels.iterrows():\n",
        "        key = (row['Dst IP'], int(row['time_window']))\n",
        "        label = int(row['label']) if pd.notnull(row['label']) else 0\n",
        "        label_map[key] = max(label_map.get(key, 0), label)\n",
        "\n",
        "    # 3. Préparer les dimensions\n",
        "    residuals_np = tl.to_numpy(residuals)\n",
        "    idx_to_dst_ip = {idx: ip for ip, idx in mappings['Dst_IP'].items()}\n",
        "    idx_to_feature = {idx: feat for idx, feat in mappings['features_names'].items()}\n",
        "    time_dim, _, dst_dim, feat_dim = residuals_np.shape\n",
        "\n",
        "    # 4. Construire la liste des time_windows à partir de l’index\n",
        "    time_windows = list(range(time_dim))\n",
        "\n",
        "    # 5. Aggrégation + étiquetage\n",
        "    aggregation = {}\n",
        "    for t in range(time_dim):\n",
        "        time_window = time_windows[t]\n",
        "        for d in range(dst_dim):\n",
        "            # Check if the index 'd' exists in idx_to_dst_ip\n",
        "            if d in idx_to_dst_ip:\n",
        "                dst_ip = idx_to_dst_ip[d]\n",
        "                key = (dst_ip, time_window)\n",
        "\n",
        "                if key not in aggregation:\n",
        "                    aggregation[key] = {\n",
        "                        'Dst IP': dst_ip,\n",
        "                        'time_windows': time_window,\n",
        "                        'count': 0,\n",
        "                        'bytes': 0,\n",
        "                        'packets': 0,\n",
        "                        'duration': 0,\n",
        "                        'label': label_map.get(key, 0)  # label ajouté ici\n",
        "                    }\n",
        "\n",
        "                for f in range(feat_dim):\n",
        "                    feature_name = idx_to_feature[f].lower()\n",
        "                    score = np.sum(np.abs(residuals_np[t, :, d, f]))\n",
        "\n",
        "                    if 'count' in feature_name:\n",
        "                        aggregation[key]['count'] += score\n",
        "                    elif 'bytes' in feature_name:\n",
        "                        aggregation[key]['bytes'] += score\n",
        "                    elif 'packets' in feature_name:\n",
        "                        aggregation[key]['packets'] += score\n",
        "                    elif 'duration' in feature_name:\n",
        "                        aggregation[key]['duration'] += score\n",
        "            else:\n",
        "                continue\n",
        "\n",
        "    df_scores = pd.DataFrame(list(aggregation.values()))\n",
        "\n",
        "    #log_step(\"Étape 4\", start)\n",
        "    print(\"✅ Aggrégation des scores + labels terminée.\")\n",
        "\n",
        "    return df_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "x65CBMk0ReBi"
      },
      "outputs": [],
      "source": [
        "mappings={'Dst_IP':dst_ip_to_idx,'features_names':dict(enumerate(['count','bytes','packets','duration']))}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "p3naVxKNReBj"
      },
      "outputs": [],
      "source": [
        "def remove_outliers_iqr(df, features):\n",
        "    cleaned_df = df.copy()\n",
        "    for col in features:\n",
        "        Q1 = cleaned_df[col].quantile(0.25)\n",
        "        Q3 = cleaned_df[col].quantile(0.75)\n",
        "        IQR = Q3 - Q1\n",
        "        lower_bound = Q1 - 1.5 * IQR\n",
        "        upper_bound = Q3 + 1.5 * IQR\n",
        "        # Filtrer les lignes en fonction de la colonne actuelle\n",
        "        cleaned_df = cleaned_df[(cleaned_df[col] >= lower_bound) & (cleaned_df[col] <= upper_bound)]\n",
        "    return cleaned_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GT1_hToQAPI",
        "outputId": "d0767eac-bb62-46ea-d211-343f3c8be911"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Étape 4 : Aggrégation des scores avec labels...\n",
            "✅ Aggrégation des scores + labels terminée.\n"
          ]
        }
      ],
      "source": [
        "df_scores_train_9=aggregation_scoring_Source(residuals_train,mappings, df_train, window_size=300)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vy__7T0R7jw",
        "outputId": "ff275eb8-3faf-485c-ab58-740582ac0b14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Outliers supprimés.\n"
          ]
        }
      ],
      "source": [
        "df=df_scores_train_9\n",
        "features=['count','bytes','packets','duration']\n",
        "removed=remove_outliers_iqr(df_scores_train_9,features)\n",
        "print(\"✅ Outliers supprimés.\")\n",
        "df_cleaned=pd.concat([removed,df[df['label']==1]],axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "efjMWnKGTlEn"
      },
      "outputs": [],
      "source": [
        "def build_and_train_binary_classifier(X_train, y_train, X_test, y_test, learning_rate=0.0001, batch_size=32, epochs=50):\n",
        "    input_dim = X_train.shape[1]\n",
        "\n",
        "    # Définition du modèle\n",
        "    model = models.Sequential([\n",
        "        layers.Input(shape=(input_dim,)),\n",
        "        layers.Dense(128, activation='sigmoid'),\n",
        "        layers.Dense(80, activation='sigmoid'),\n",
        "        layers.Dense(80, activation='sigmoid'),\n",
        "        layers.Dense(32, activation='sigmoid'),\n",
        "        layers.Dense(32, activation='sigmoid'),\n",
        "        layers.Dropout(0.3),\n",
        "        layers.Dense(64, activation='sigmoid'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='sigmoid'),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "\n",
        "    ])\n",
        "\n",
        "    # Compilation\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy', tf.keras.metrics.AUC()]\n",
        "    )\n",
        "\n",
        "\n",
        "    # Entraînement\n",
        "    history = model.fit(\n",
        "        X_train, y_train,\n",
        "        validation_data=(X_test, y_test),\n",
        "        batch_size=batch_size, # Reduced batch size\n",
        "        epochs=epochs,\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "_S5uWSzyaJAK"
      },
      "outputs": [],
      "source": [
        "scale=StandardScaler()\n",
        "X=df_cleaned.drop(columns=['label','Dst IP','time_windows'])\n",
        "Y=df_cleaned['label']"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X1=scale.fit_transform(X)\n",
        "Y1=df_cleaned['label']\n",
        "X1_train,X1_test,Y1_train,Y1_test = train_test_split(X1,Y1,test_size=0.3,stratify=Y1)\n",
        "model,history=build_and_train_binary_classifier(X1_train, Y1_train, X1_test, Y1_test)\n",
        "\n",
        "\n",
        "# Prédire les probabilités (entre 0 et 1)\n",
        "y_pred_prob = model.predict(X1_test)\n",
        "      # Convertir les probabilités en classes binaires (0 ou 1)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "      # Calcul de la matrice de confusion\n",
        "cm = confusion_matrix(Y1_test, y_pred)\n",
        "      # Affichage\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "disp.plot(cmap='Blues')\n"
      ],
      "metadata": {
        "id": "6tOm00v4zYNR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "residuals_test=latent_space_projection(tensor_test,factors_normal)"
      ],
      "metadata": {
        "id": "6WQoE4UYzdPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FWEvpZU2VQ6P",
        "outputId": "487ef4cc-f9ca-4d79-c1df-6f66ff0e2e87"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "⏳ Étape 4 : Aggrégation des scores avec labels...\n",
            "✅ Aggrégation des scores + labels terminée.\n",
            "✅ Score calculé.\n"
          ]
        }
      ],
      "source": [
        "df_scores_test=aggregation_scoring_Source(residuals_test,mappings, df_test, window_size=300)\n",
        "plt.show()\n",
        "print(\"✅ Score calculé.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VMOleaBfU_4p",
        "outputId": "5f3e6aad-7c92-4176-fd1f-3103e894d90d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Outliers supprimés.\n"
          ]
        }
      ],
      "source": [
        "removed_test=remove_outliers_iqr(df_scores_test,features)\n",
        "print(\"✅ Outliers supprimés.\")\n",
        "df_cleaned_test=pd.concat([removed_test,df_scores_test[df_scores_test['label']==1]],axis=0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoQYHIF8Ovri"
      },
      "outputs": [],
      "source": [
        "\n",
        "      X=df_scores_test.drop(columns=['label','Dst IP','time_windows'])\n",
        "      X_scaled=scale.fit_transform(X)\n",
        "      Y=df_scores_test['label']\n",
        "      y_pred_prob = model.predict(X_scaled)\n",
        "      # Convertir les probabilités en classes binaires (0 ou 1)\n",
        "      y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "      # Calcul de la matrice de confusion\n",
        "      cm = confusion_matrix(Y, y_pred)\n",
        "      # Affichage\n",
        "      disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "      disp.plot(cmap='Blues')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}